# MLIR：在机器学习框架中逐步应用图算法

有关 MLIR 的现有文献侧重于长期愿景，即如何将各个部分整合在一起，以及模块化和可组合基础设施在广阔而遥远的未来所带来的好处。这种观点吸引了一些人，但也引起了另一些人的担忧，因为他们更关注“此时此地”。既然任何单个问题都可以就地解决，为什么还要做出“革命性”的改变呢？

本文档解释了采用 MLIR 解决基于图的问题并不是革命性的变革：它是一系列渐进的步骤，这些步骤相互依存，每个步骤都能带来局部价值。本文档还解决了不断出现的一些困惑。

需要注意的是：尽管 MLIR 的主要优势在于它可以涵盖从图算法到底层代码生成的所有领域，但本文档的重点是将 MLIR 用于**图级算法**。MLIR 还将带来令人兴奋的代码生成机会（特别是考虑到其集成最先进的多面体技术的新方法），但涉及 MLIR 与 XLA、Eigen 等的关系的问题不在本文档的讨论范围之内。

本文档以 TensorFlow 为例，因为它是我们当前工作的重点，但我们相信，对于在其他 ML 框架背景下工作的人，同样的观点可能很有用，这些框架将来可能会考虑采用 MLIR。

### MLIR是关于什么的？

MLIR 是一个重载的首字母缩写词，拆开后就是“Multi-Level Intermediate Representation”。它的高层次目的是提供以灵活方式描述和变换程序与计算的机制。它为常量折叠、死代码消除、图重写等提供了通用的编译器基础设施，而这些基础设施与特定方言所选择的表示行为（例如其并发语义）无关。它在构建时特别注重编译时间和内存效率、源位置信息的准确传播（这对报告高质量错误和警告非常重要），并专为可测试性而设计。

TensorFlow 有许多子系统（其中一些是专有的，如 Tensor-RT、nGraph、CoreML 等）以及这些不同子系统之间的翻译层，而这些翻译层也面临着类似的挑战。(顺便提一句，这些子系统的内部结构通常都能从 MLIR 基础设施中受益，但这并不是本文档的重点）。

MLIR 提出的一个关键观点是，这些子系统通常有两个方面：它们既是特定的数据结构和编码（例如 HLO 图、TF-Lite 的平面缓冲区格式、TensorFlow 的图格式、ONNX 抽象等），也是计算的抽象（一种特定的卷积建模方式、一组受支持的操作等）。

MLIR 使用标准 IR（即一组数据结构）来表示这些计算。这样就可以在这些问题领域中共享大量基础设施。然后，MLIR 允许定义特定领域的“方言”，以描述特定应用合法且支持的操作集。这意味着数据结构之间的实际翻译尽可能简单，因此相对容易做到“正确”。这样，通用编译器基础设施就可以处理映射问题和领域内的其他问题。

MLIR 的设计直接借鉴了 LLVM IR、LLVM SelectionDAG、LLVM 机器指令表示法、Swift SIL IR 等中间表示法的构建（和使用）经验，并从 TensorFlow 和 XLA HLO 中吸取了新的教训，还从在它们之上构建的无数研究和生产系统中汲取了经验。我们的目标是推动编译器技术的发展，而不仅仅是将一些众所周知的技术应用到机器学习领域。

### 采用MLIR意味着什么？

本文档的重点并不是提倡重写 TensorFlow 中的任何特定子系统。事实上，证明重写的合理性所需的负担很高，而且通常非常特定于该子系统。尽管如此，还是有几个子系统即将被重写或大幅修改，因此我们以这些子系统为例，具体描述 MLIR 在这些情况下提供的优势以及需要采取的措施。讨论的子系统包括：

1. TF Lite TOCO 翻译器，我们需要改进错误报告/可靠性问题，并使其支持更多操作，以及
2. TF/XLA 桥接器，它需要通过合并一些用法模型来提高可用性，支持动态形状，并将对客子系统的支持推广到 Tensor-RT 和 nGraph。
3. Grappler 是另一个子系统，未来可能会进行大幅修改，MLIR 框架肯定会使其受益，但目前还没有开展这项工作的已知计划，因此我们不做进一步讨论。

采用 MLIR 进行这些工作的方式是一样的。事实上，支持 TF Lite 的工作主要是支持 TF/XLA 桥接功能的大型工作的一个子集。TF Lite 和 TF/XLA 桥接器包括多个编译器passes（如封装、控制流功能化、操作的降级、融合、常量折叠、形状推理等）。

MLIR 支持从 TensorFlow Graphs 转换到 MLIR，然后再转换回来，这意味着我们可以先将 TensorFlow 图进行一个无操作（no - op）的转换，转换为 MLIR，然后再转回 TensorFlow 图，并将其放入处理管线中，验证整个过程是否正常（即没有任何问题）。一旦验证通过，我们就可以开始逐步替换编译器变换过程，通过重新实现这些变换（使用我们计划中的改进算法）来逐步替换它们。

这只是一个开发计划，我们实际上不会发布仅使用 MLIR 作为单个pass的 TensorFlow。在实践中，我们会将 MLIR 标志设置为一个选项，为整个子系统（如 TOCO 翻译器）构建一个替代方案，在时机成熟时，我们会进行 A/B 比较，最终进行切换，并逐步淘汰旧代码。

## MLIR能带来什么好处？

上述采用计划听起来似乎只会在短期内让事情变得更糟。我们有两个相同功能的实现，我们正在分散投入，等等。为了让这一切物有所值，我们应该清楚地认识到，我们正在建设一个更好的未来，当它到来时，客户和 TensorFlow 工程师都会更高兴。下面我们将介绍 MLIR 带来的一些好处，排名不分先后：

### 无损的人类可编辑文本表示法

MLIR 内存中数据结构具有人类可读和可写的格式以及其[规范](https://mlir.llvm.org/docs/LangRef/)，就像其他编程语言一样构建。这种格式的重要特性是紧凑、易读和无损。你可以将一个 MLIR 程序转储到磁盘上，然后对其进行处理，让其通过更多的passes。

如果你没有使用过这种工作方式的系统，那么很难夸大这在实践中的重要性：这意味着你可以在 IR 对象上调用 `foo->dump()` 查看其全部内容，这意味着你可以对更改前后的 IR 进行比较，对 IR 文件进行 delta 缩减，以及其他许多事情。

### 图验证Pass

与许多其他流行的编译器基础设施一样，MLIR 为“验证器”提供了基础设施和实现，“验证器”可以检查 IR 的格式是否正确。MLIR 验证器是一个简单的框架，可以轻松地为这些正确性属性提供单一事实来源，并且适用于所有方言（如 TF Graph、TF Lite flat buffer、XLA HLO 等）。

验证器pass有点像“超级断言”，它能及早发现程序变换中的错误，从而提高工程师的工作效率，使产品更可靠，并在出现错误时更容易追踪。因为验证器可以随时运行，既可以作为编译器pass，也可以通过单个函数调用运行。

虽然 MLIR 为IR验证提供了一个经过深思熟虑的基础设施，并对现有的 TensorFlow 操作进行了简单的检查，但这里还有很多需要添加的内容，也有很多参与的机会！

### 为可测试性而设计

在 MLIR 中，这体现在很多方面，但我们将重点关注编译器变换，因为它们最容易理解。编译器变换被建模为 `Pass` C++ 类的子类，由 `mlir-opt` 工具驱动。当与无损文本表示相结合时，为编译器变换编写单元测试就变得非常容易了，例如，这是一个简单的测试，可以显示“x-x”被转化为零：

```mlir
  // RUN: mlir-opt %s -canonicalize | FileCheck %s
  func.func @test_subi_zero_cfg(%arg0: i32) -> i32 {
    %y = arith.subi %arg0, %arg0 : i32
    return %y: i32
  }
  // CHECK-LABEL: func @test_subi_zero_cfg(%arg0: i32)
  // CHECK-NEXT: %c0_i32 = arith.constant 0 : i32
  // CHECK-NEXT: return %c0
```

“CHECK”注释由[LLVM FileCheck 工具](https://llvm.org/docs/CommandGuide/FileCheck.html)解释，它有点像一个非常高级的grep。该测试是完全独立的：它将输入送到[canonicalize pass](https://mlir.llvm.org/docs/Canonicalization/)中，并检查输出是否与 CHECK 行匹配。更多示例请参见 `test/Transforms` 目录。相比之下，标准单元测试会将底层框架的 API 暴露给大量的测试（使得重构和移动 API 变得更加困难），通常需要更多的代码，并会加剧链接时间的问题。有关示例，请参阅[TensorFlow 测试套件中的 TEST_F 函数](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/core/grappler/optimizers/arithmetic_optimizer_test.cc)。

MLIR 在设计时普遍采用了这种可测试性设计，这让我们能够建立一种惯例，期望每一次行为改变的提交都包含一个测试用例，而且这些测试用例随着时间的推移都能保持稳定和可靠，因为它们测试的正是它们应该测试的。当然，端到端的集成测试对某些事情还是非常有用的！

### 用于警告、错误诊断和位置跟踪的基础设施

MLIR 从构建其他编译器的经验教训中获益匪浅，其中包括 Clang，它为 C/C++ 编译器诊断的实现质量[设定了标准](http://blog.llvm.org/2010/04/amazing-feats-of-clang-error-recovery.html)。借鉴这些经验（并修正 LLVM 中的错误），MLIR 要求操作和函数携带抽象的位置信息，要求变换传播这些信息，并提供标准化机制来发出错误和警告，以及让客户端挂接这些机制，以自定义方式捕获和报告错误和警告。

为什么这很重要？在实践中，许多图到图的翻译器都可能出现故障（例如，当使用了不支持的操作时，TF Lite 就会出现故障），因此必须能够以最精确的方式向用户报告错误，从而使其具有可操作性。这包括通过操作的融合和裂变来跟踪重写、映射回语言/API 特定域等。

对于基础设施开发人员来说，这更是一种巨大的好处，因为这意味着很容易就能写出好的测试：MLIR 的测试工具会捕获passes（使用标准诊断钩子）产生的诊断结果，并检查它们是否与测试用例中的预期诊断结果相匹配。例如，为了测试代码生成器中的依赖分析基础设施，Andy Davis 写了一个简单的pass，用于检查依赖关系并以“notes”的形式生成结果，这样他就可以写出如下的测试了：

```mlir
  // RUN: mlir-opt %s -memref-dependence-check -verify-diagnostics
  func.func @different_memrefs() {
    %m.a = memref.alloc() : memref<100xf32>
    %m.b = memref.alloc() : memref<100xf32>
    %c0 = arith.constant 0 : index
    %c1 = arith.constant 1.0 : f32
    memref.store %c1, %m.a[%c0] : memref<100xf32>
    // expected-note@-1 {{dependence from memref access 0 to access 1 = false}}
    %v0 = memref.load %m.b[%c0] : memref<100xf32>
    return
  }
```

请注意，这样做的一个主要限制是，MLIR 存在“垃圾输入，垃圾输出”的问题：如果 MLIR 的输入位置不精确，那么它就无法执行任何措施来恢复它们。TensorFlow/Python 正在努力改善这种情况，而Swift for TensorFlow在设计上已经具备了完美的位置跟踪能力。

### 在IR中捕获的形状信息

在 TensorFlow Graphs 中，每个操作都使用一个非常简单的类型系统（TF_DataType）来获取和返回值，其中每个值都是一个未知秩和维度的张量。与此同时，许多图的静态形状对于大部分计算都是可知的，即使是动态形状的操作也往往具有静态可知的维度。许多分析和变换都会受益于并使用这些可用信息，但由于 TensorFlow 计算图不会捕获此信息（例如将其序列化为 proto），因此passes必须使用 ShapeRefiner 按需重新计算它。

[MLIR 张量类型](https://mlir.llvm.org/docs/Dialects/Builtin/#rankedtensortype)可以直接捕获形状信息，因此你可以使用类似的功能：

```mlir
  %x = tf.Add %x, %y : tensor<128 x 8 x ? x f32>
```

在 IR 中捕获此信息有望加快变换速度（避免重复计算相同的信息），从而使应用更强的形状分析算法变得更加可行。此外，这也使得与IR的交互变得更加容易，因为旁侧表示可能会过时，而且从人体工程学的角度来看，API 也更易于使用。

### 统一图重写基础设施

这项工作仍在进行中，但我们已经看到了[通用重写基础设施](https://mlir.llvm.org/docs/Rationale/RationaleGenericDAGRewriter/)的雏形，它可以使用声明性模式的格式将 DAG 图块变换为其他 DAG 图块。DAG 到 DAG 的重写是许多常见编译器优化、降级和其他重写的通用解决方案，有了 IR，我们就能投入努力构建一个高质量的实现。

声明性模式规则比命令式 C++ 代码更可取，原因有很多：声明性模式规则更紧凑、更容易推理、可以针对它们编写检查程序，而且可以构建新的工具，以有趣的方式检查和处理声明性模式。例如，将定理证明器应用于声明性模式。随着基础设施的成熟，这一生态的发展将令人激动。

### 为TensorFlow操作明确语义

使用 TensorFlow 所面临的挑战之一是，在使用图时需要保留和了解许多不变量和行为，而这些不变量和行为可能难以推理并导致错误。诸如“死值”、切换和合并节点、并发语义、即使传递死值也能执行的节点、多设备程序表示法等等。所有这些都增加了复杂性，使得推理变换或分析总体上是否正确变得具有挑战性。即使是像常量折叠或将整数`x-x`变换为`0`这样简单的事情也并非易事，因为你需要考虑控制依赖边。

我们开发 MLIR 的 TensorFlow 方言的主要目标之一就是理清这些情况，并将现有的 TensorFlow 计算图升级为更易于推理的语义。这些问题的解决方案仍在争论之中，但这些讨论已经产生了许多潜在的答案：为 switch/merge 引入 `tf_dead_or<x>` 类型，使用 futures/async 语义对 TF 操作进行建模等。对于 MLIR 的成功而言，这些特定的争论都不是关键或重要的（由于其“元”性质，任何给定方言的抽象决策都要由它来决定），但每一次成功的争论都将使 TensorFlow 操作的使用和变换变得更容易。我们希望这些问题能在未来几个月内得到解决，届时 MLIR 的工作将超越 TF Lite / TOCO 支持。目前正在进行的讨论非常有价值，并且正在取得进展。

### 人体工程学

理论上不重要，但在实践中却很重要的一点是，MLIR 的设计旨在使代码变换比其他系统更简单、更节省内存、更不易出错。`TensorFlow::Graph`在实现上存在一些问题，比如相同的信息被冗余地存储在不同的地方（这些地方必须手动保持更新），对某些结构的表示有些不寻常（例如函数库，这使得添加或删除函数变得非常困难，例如在程序间变换时），并且在图中存储了执行器使用的信息，但这些信息并非程序变换所必需的。

多年来，TensorFlow 在这一领域取得了长足进步，我们对未来的进一步改进也有很多想法，我们很高兴 MLIR 如今能满足这些需求（使正确的程序变换更容易实现），并致力于努力使其变得更好。

### 编译时性能和内存使用

MLIR 在设计时就注重其算法和数据结构的内存效率和编译时效率。它采用了不可变和唯一化的结构、低级位打包以及其他一些广为人知的技术手段，以此来避免不必要的堆内存分配，并允许对 MLIR 程序进行简单安全的多线程优化。考虑到 TensorFlow 目前的实现细节，我们还有其他理由相信，常见变换的 MLIR 实现将比相同内容的 Python 和 C++ TensorFlow::Graph 实现更高效。

尽管如此，目前这在很大程度上还只是一种理论。当各种子系统的新实现可用时，我们将看到实践中会发生什么：没有理由去猜测，我们可以测量。

## 常见问题和疑虑

我们在此讨论一些常见问题和疑虑。

### MLIR不是一个很大的依赖吗？

我们听说至少有一些人担心 MLIR 是一个“大”依赖，会导致代码量过大。以下是 MLIR 的一些要点：

1. 与现代 ML 框架相比，整个 MLIR 代码库绝对是一个相当小的 C++ 代码库。
2. 与 LLVM 类似，MLIR 被设计为一套库，客户可以随意链接或忽略。例如，MLIR 中的变换与核心 IR 抽象是分开的，方言特定代码（如 TensorFlow、TF-Lite、XLA 等）都可由构建系统独立选择。不管是 TF-Lite 系统还是与 TensorFlow 完全无关的系统，不关心 XLA 的客户都不会链接该代码。
3. MLIR 唯一的第三方依赖是 LLVM，但它并不依赖于 LLVM IR 或任何其他重度依赖。它只是依赖于 LLVM 的支持库，该库提供了高效的哈希表和其他[STL 所不具备的内存高效数据结构](http://llvm.org/docs/ProgrammersManual.html#picking-the-right-data-structure-for-a-task)。人们一直在讨论将这套库拆分为 LLVM 中自己的子项目，而 LLVM IR 项目则依赖于该子项目。这对 MLIR 和其他 LLVM 子项目都有好处。
4. TensorFlow 和许多其他框架已经使用了 LLVM。如果是这样的话，MLIR 就完全不需要额外的依赖了。

### MLIR如何在TensorFlow中表示{control flow, concurrency, …}语义？

MLIR 提供了一种方言，它是 TensorFlow 计算图与 MLIR 之间的同构 1-1 映射，也是一个相当完整的双向翻译器（唯一已知的差距是还没有处理一些 TF_DataType 枚举）。MLIR 是一种“多级IR”，它可以表示不同抽象层次的代码，因此以完全向后兼容的方式忠实表示 TensorFlow 代码的能力至关重要（即使存在一些历史缺陷！）。

除了同构映射之外，我们还在积极努力提高在 MLIR 中处理 TensorFlow 计算图的抽象级别。这样做将使 TensorFlow 变换的编写变得比现在更容易，并为 TF 1.x 图迁移到 TF 2.x 的世界提供了途径。例如，由于 MLIR 具有可扩展的类型系统，我们可以直接模拟 Tensor 值是否不可能成为“死”值，类似于现代编程语言中可选类型的使用。

这些讨论偶尔会引起混淆，因为有几个问题被混为一谈：

- TensorFlow 图的当前语义是什么？我们可以依赖哪些不变量？
- TensorFlow 2.0 的语义应该是什么？
- 程序在实践中依赖什么？如果它不友好，我们能否将其迁移？
- 我们能否找到一种方法，通过使用更高级别的控制流表示，让变换不必担心 Switch/Merge 的复杂性？(暂定答案：可以）
- MLIR 应如何表示异步与同步操作，提供了哪些不变量，如何与控制流对接？
- 何时执行可能降低并行性的优化是安全和有益的？

所有这些问题都有一个“保守/安全的后备方案”：我们可以继续提供与 TensorFlow 完全相同的抽象。尽管如此，我们仍在努力提高表征的层次（利用 MLIR 的“多层次”部分），因为这样做会比目前在 TensorFlow 中更容易编写分析和变换。

### 非目标

有必要指出 MLIR 的非目标。例如，MLIR 没有运行时组件：TensorFlow 执行器、TF Lite FlatBuffer 解释器或其他现有运行时应按原样使用。

另一个非目标是，MLIR 目前不支持稳定的二进制编码。我们肯定会在某个时候添加这一功能，但在此期间，应使用现有格式进行序列化和分发。